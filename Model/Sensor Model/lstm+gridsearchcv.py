# -*- coding: utf-8 -*-
"""LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fGJ2_cVPsWGpI-3C_6fOhdqB-_hJURtQ
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from scikeras.wrappers import KerasClassifier
from tensorflow.keras.layers import Input, LSTM, Dense, Dropout
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder

data1 = pd.read_csv('/content/sensor_data_nRF_IMU_1_19092025_164551_not_labelled.csv')
data2 = pd.read_csv('/content/sensor_data_nRF_IMU_2_19092025_164551_labelled.csv')

data1.head()

data2.head()

data2 = data2.drop('Unnamed: 11', axis=1)
data2.head()

data2.dropna(inplace=True)
data2.head()

data = pd.merge(data1, data2, on=['TimeStamp'])
data.head()

data.to_csv('19092025_labelled.csv', index=False)

data.info()

X = data.drop(['Position','TimeStamp'], axis=1).values
y = data['Position'].values

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

y

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

def create_sequences(X, y, seq_len):
    Xs, ys = [], []
    for i in range(len(X) - seq_len):
        Xs.append(X[i:(i + seq_len)])
        ys.append(y[i + seq_len])
    return np.array(Xs), np.array(ys)

def create_lstm_model(
    sequence_length=50,
    num_layers=2,
    lstm_units_1=128,
    lstm_units_2=64,
    dropout_rate=0.3,
    dense_units=64,
    activation='relu',
    optimizer='adam',
    loss='sparse_categorical_crossentropy'
):
    model = Sequential()
    model.add(Input(shape=(sequence_length, X.shape[1])))
    model.add(LSTM(lstm_units_1, return_sequences=(num_layers > 1)))
    model.add(Dropout(dropout_rate))
    if num_layers > 1:
        model.add(LSTM(lstm_units_2))
        model.add(Dropout(dropout_rate))
    model.add(Dense(dense_units, activation=activation))
    model.add(Dense(len(np.unique(y)), activation='softmax'))
    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])
    return model

param_grid = {
    'model__sequence_length': [40, 50, 60],
    'model__num_layers': [1, 2],
    'model__lstm_units_1': [64, 128],
    'model__lstm_units_2': [32, 64],
    'model__dropout_rate': [0.3, 0.5],
    'model__dense_units': [32, 64],
    'model__activation': ['relu', 'tanh'],
    'model__optimizer': ['adam', 'rmsprop'],
    'model__loss': ['sparse_categorical_crossentropy', 'categorical_crossentropy'],
    'batch_size': [32, 64],
    'epochs': [15, 30]
}

def build_sequences_for_grid(sequence_length):
    X_seq, y_seq = create_sequences(X, y, sequence_length)
    return train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)

best_score = 0
best_params = None

for sequence_length in param_grid['model__sequence_length']:
    X_seq, y_seq = create_sequences(X, y, sequence_length)
    X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)
    grid = GridSearchCV(
        KerasClassifier(model=create_lstm_model, verbose=1),
        param_grid,
        scoring='accuracy',
        cv=3,
        n_jobs=1
    )
    grid_result = grid.fit(X_train, y_train)
    print(f"Best score for sequence_length={sequence_length}: {grid_result.best_score_}")
    print(f"Best params: {grid_result.best_params_}")
    if grid_result.best_score_ > best_score:
        best_score = grid_result.best_score_
        best_params = grid_result.best_params_

print(f"Overall Best score: {best_score}")
print(f"Overall Best Parameters: {best_params}")

print("Thank You")